args <- c('./input/rep_time.txt', './output/test') # test input (comment out after test)
del_array <- fread(file = "./input/del.txt", sep = "\t", nThread = 2) # deletion array
input <- read.csv(file = args[1], sep = "\t") # trait data
dat.use <- merge(input, del_array, by = "ID") # merge data (deletion array and trait data)
dat.use <- na.omit(dat.use)
dat.use[, 2] <- scale(dat.use[, 2])
setwd("~/iTARGEX")
args <- c('./output/all_random/') # test input (comment out after test)
subdirs <- dir(path = args[1], pattern = "output_*", full.names = TRUE)
num_of_replicate = length(subdirs)
df <- data.frame()
for (i in 1: num_of_replicate) {
next_df <- read.csv(file.path(subdirs[i], "cor_sig_local.csv"))
df = rbind(df, next_df)
}
colnames(df)[1] <- c("gene")
View(df)
observed_genes <- unique(df$gene)
summary_df <- c()
for (i in 1: length(observed_genes)) {
current_df <- subset(df, df$gene == observed_genes[i], select = c(-1))
observed_prob <- nrow(current_df) / num_of_replicate
names(observed_prob) <- c("prob")
new_df_mean <- apply(current_df, 2, mean)
new_df_sd <- apply(current_df, 2, sd)
new_df_comb <- do.call(cbind, Map(cbind, new_df_mean, new_df_sd))
names(new_df_comb) <- do.call(c, Map(cbind, names(new_df_mean), paste0(names(new_df_sd), "_sd")))
new_df_comb <- c(observed_prob, new_df_comb)
summary_df <- rbind(summary_df, new_df_comb)
}
summary_df <- as.data.frame(summary_df)
rownames(summary_df) <- observed_genes
summary_df <- summary_df[order(summary_df$pval_beta), ]
format(summary_df, digits = 4)
format(summary_df, digits = 4, scientific = FALSE)
library(data.table)
library(foreach)
library(doParallel)
library(scales)
setwd("~/iTARGEX/output/all_random")
args <- c('../../input/rep_time.txt', './output_5') # test input (comment out after test)
del_array <- fread(file = "../../input/del.txt", sep = "\t", nThread = 2) # deletion array
input <- read.csv(file = args[1], sep = "\t") # trait data
dat.use <- merge(input, del_array, by = "ID") # merge data (deletion array and trait data)
dat.use <- na.omit(dat.use)
dat.use[, 2] <- scale(dat.use[, 2])
x <- dat.use[, 2]
y <- dat.use[, -c(1, 2)]
output_dir <- args[2]
param <- read.csv(file = file.path(output_dir, "local_param.csv"), row.names = 1)
weight_c1 <- as.matrix(fread(file = file.path(output_dir, "weight_comp1.csv"), nThread = 2))
weight_c2 <- as.matrix(fread(file = file.path(output_dir, "weight_comp2.csv"), nThread = 2))
#### correlation computation ####
# p-value computation based on t-test
t_test_pval <- function (n, r) {
pval_tail_1 <- pt(r * sqrt(n - 2) / sqrt(1 - r ^ 2), n - 2)
pval_tail_2 <- 1 - pval_tail_1
2 * min(pval_tail_1, pval_tail_2)
}
test_beta <- function(x, y, weights = NULL) {
a <- lm(y ~ x, weights = weights)
summary(a)$coefficients[2, c(1, 4)]
}
#### function for combine the results from parallel processes ####
comb <- function(x, ...) {
mapply(rbind, x, ..., SIMPLIFY=FALSE)
}
cl <- makeCluster(4)
registerDoParallel(cl)
cor_df <- foreach(i = 1: ncol(y), .combine = 'comb', .multicombine = TRUE) %dopar% {
# for (i in 1: ncol(y)) {
cor_local <- c(1: 12) * NA
# Skip cases that is not convergent or extremely few in one of the component
if (param[i, "conv"] == "N" || param[i, "lambda_1"] < 0.05 || param[i, "lambda_2"] < 0.05) {
return(cor_local)
}
# Soft assign
cor_comp1 <- cor(x * weight_c1[i, ], y[, i] * weight_c1[i, ])
cor_comp2 <- cor(x * weight_c2[i, ], y[, i] * weight_c2[i, ])
# pval_cor_comp1 <- t_test_pval(n = sample_num, r = cor_comp1)
# pval_cor_comp2 <- t_test_pval(n = sample_num, r = cor_comp2)
pval_comp1 <- test_beta(x, y[, i], weights = weight_c1[i, ])
pval_comp2 <- test_beta(x, y[, i], weights = weight_c2[i, ])
cor_local[1:6] <- c(cor_comp1, pval_comp1, cor_comp2, pval_comp2)
# Hard assign
comp1_idx <- which(weight_c1[i, ] > 0.5)
comp2_idx <- which(weight_c2[i, ] > 0.5)
cor_comp1 <- cor(x[comp1_idx], y[, i][comp1_idx])
cor_comp2 <- cor(x[comp2_idx], y[, i][comp2_idx])
# pval_cor_comp1 <- t_test_pval(n = length(comp1_idx), r = cor_comp1)
# pval_cor_comp2 <- t_test_pval(n = length(comp2_idx), r = cor_comp2)
pval_comp1 <- test_beta(x[comp1_idx], y[, i][comp1_idx])
pval_comp2 <- test_beta(x[comp2_idx], y[, i][comp2_idx])
cor_local[7:12] <- c(cor_comp1, pval_comp1, cor_comp2, pval_comp2)
return(cor_local)
}
stopCluster(cl)
cor_df <- as.data.frame(cor_df, row.names = colnames(y)[1: ncol(y)])
colnames(cor_df) <- c("soft_cor_c1", "soft_beta_c1", "soft_pval_c1",
"soft_cor_c2", "soft_beta_c2", "soft_pval_c2",
"hard_cor_c1", "hard_beta_c1", "hard_pval_c1",
"hard_cor_c2", "hard_beta_c2", "hard_pval_c2")
# Apply Bonferroni correction to p-value
pval_adj_df <- sapply(cor_df[, c(3, 6, 9, 12)], p.adjust, method = "bonferroni")
colnames(pval_adj_df) <- c("soft_adjp_c1", "soft_adjp_c2", "hard_adjp_c1", "hard_adjp_c2")
cor_df <- cbind(cor_df, pval_adj_df)
# -log10(p-value) transformation
logp_df <- sapply(cor_df[, c(13: 16)], log10) * (-1) # apply -log10 transformation to dataframe
logp_df[is.infinite(logp_df)] <- 350 # Set the p-value with infinite value to 350
cor_df[, c(13: 16)] <- logp_df
# Concatenate more information to the dataframe
cor_df <- cbind(cor_df, param$lambda_1, param$lambda_2)
colnames(cor_df)[c(17, 18)] <- c("ratio_1", "ratio_2")
View(cor_df)
rownames(cor_df)
which(rownames(cor_df) == "ADA2")
cor_df[645,]
param[645,]
